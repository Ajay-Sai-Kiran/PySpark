{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis in PySpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM75gvZiQAN+kPsen3y9y7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaysaikiran2208/PySpark/blob/main/Sentiment_analysis_in_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5si3FaItyP"
      },
      "source": [
        "#Sentiment analysis in PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vFyZ9vjj8Hm",
        "outputId": "5df45730-fb76-4148-d3ec-db4964a71d0c"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 59kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=f609662af2d3bda4d05b2d4e6614e065e1e37919beb97470b3204e83a082089e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KFJfu6VG1Py"
      },
      "source": [
        "#Import modules and create spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTgb-zWhMGq"
      },
      "source": [
        "#import modules\n",
        "from pyspark.sql.types import * #PySpark SQL Types class is a base class of all data types in PySpark which defined in a package pyspark. sql. types. DataType and they are used to create DataFrame with a specific type.\n",
        "from pyspark.sql.functions import * #List of built-in functions available for DataFrame.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#create Spark session\n",
        "appName = \"Sentiment Analysis in Spark\"\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(appName) \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvKkob_-I8GX"
      },
      "source": [
        "# Read data file into Spark dataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ1cbPd5kA0z",
        "outputId": "ee296dc0-341a-4f08-f5e4-8b8c955a2558"
      },
      "source": [
        "#read csv file into dataFrame with automatically inferred schema\n",
        "tweets_csv = spark.read.csv('/content/tweets.csv', inferSchema=True, header=True)\n",
        "tweets_csv.show(truncate=False, n=3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---------+---------------+---------------------------------+\n",
            "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
            "+------+---------+---------------+---------------------------------+\n",
            "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
            "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
            "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
            "+------+---------+---------------+---------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq-UdVhxJB7V"
      },
      "source": [
        "#Select Relevant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7CyUE2Zkwo9",
        "outputId": "d5f706e8-f771-4007-abaf-68817648c8a9"
      },
      "source": [
        "\n",
        "#select only \"SentimentText\" and \"Sentiment\" column, \n",
        "#and cast \"Sentiment\" column data into integer\n",
        "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
        "data.show(truncate = False,n=5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------+-----+\n",
            "|SentimentText                    |label|\n",
            "+---------------------------------+-----+\n",
            "|that film is fantastic #brilliant|1    |\n",
            "|this music is really bad #myband |1    |\n",
            "|winter is terrible #thumbs-down  |0    |\n",
            "|this game is awful #nightmare    |0    |\n",
            "|I love jam #loveit               |1    |\n",
            "+---------------------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAuNWoc5JGOV"
      },
      "source": [
        "#Splitting data into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0TGsOOsknRC",
        "outputId": "b8e9fd53-b524-4f45-b3f8-0ad33affef53"
      },
      "source": [
        "#divide data, 70% for training, 30% for testing\n",
        "dividedData = data.randomSplit([0.7, 0.3]) \n",
        "trainingData = dividedData[0] #index 0 = data training\n",
        "testingData = dividedData[1] #index 1 = data testing\n",
        "train_rows = trainingData.count()\n",
        "test_rows = testingData.count()\n",
        "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data rows: 1327 ; Testing data rows: 605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaobBAkRJL_9"
      },
      "source": [
        "#data processing and labeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IHxuzgVkzlO",
        "outputId": "3b8bf5bd-6cee-4157-e3dd-2ecf93464ce9"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
        "tokenizedTrain = tokenizer.transform(trainingData)\n",
        "tokenizedTrain.show(truncate=False, n=5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------+-----+------------------------------+\n",
            "|SentimentText            |label|SentimentWords                |\n",
            "+-------------------------+-----+------------------------------+\n",
            "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |\n",
            "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|\n",
            "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |\n",
            "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |\n",
            "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|\n",
            "+-------------------------+-----+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1MlKoCCKm7s"
      },
      "source": [
        "#Using Stopword command to remove unwanted words from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCtkWTUdk5oe",
        "outputId": "6c4eb95d-ba39-4aca-c4bb-fd813364dae5"
      },
      "source": [
        "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
        "                       outputCol=\"MeaningfulWords\")\n",
        "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
        "SwRemovedTrain.show(truncate=False, n=5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------+-----+------------------------------+---------------------------+\n",
            "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
            "+-------------------------+-----+------------------------------+---------------------------+\n",
            "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
            "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|[adore, cheese, #brilliant]|\n",
            "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
            "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
            "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
            "+-------------------------+-----+------------------------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjDpjNWKK3z1"
      },
      "source": [
        "#HashingTF\n",
        "Maps a sequence of terms to their term frequencies using the hashing trick. Currently we use Austin Appleby’s MurmurHash 3 algorithm (MurmurHash3_x86_32) to calculate the hash code value for the term object. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVzCO-R9k9kW",
        "outputId": "f4496548-c66f-4337-cf80-303c5e3e9390"
      },
      "source": [
        "\n",
        "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
        "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
        "    'label', 'MeaningfulWords', 'features')\n",
        "numericTrainData.show(truncate=False, n=3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---------------------------+-------------------------------------------+\n",
            "|label|MeaningfulWords            |features                                   |\n",
            "+-----+---------------------------+-------------------------------------------+\n",
            "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
            "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
            "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
            "+-----+---------------------------+-------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJXjIzOklBfe",
        "outputId": "4aafb963-0467-4972-ce1c-05cbcc5a2882"
      },
      "source": [
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
        "                        maxIter=10, regParam=0.01)\n",
        "model = lr.fit(numericTrainData)\n",
        "print (\"Training is done!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er8NaelMLL10"
      },
      "source": [
        "#Scaling, converting, or modifying features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYjVNWjFlFe4",
        "outputId": "6e15a896-5946-4338-8d26-be5edf6dc8dc"
      },
      "source": [
        "tokenizedTest = tokenizer.transform(testingData)\n",
        "SwRemovedTest = swr.transform(tokenizedTest)\n",
        "numericTest = hashTF.transform(SwRemovedTest).select(\n",
        "    'Label', 'MeaningfulWords', 'features')\n",
        "numericTest.show(truncate=False, n=2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-------------------------------------+--------------------------------------------------------+\n",
            "|Label|MeaningfulWords                      |features                                                |\n",
            "+-----+-------------------------------------+--------------------------------------------------------+\n",
            "|1    |[adore, classical, music, #brilliant]|(262144,[45361,100089,102383,131250],[1.0,1.0,1.0,1.0]) |\n",
            "|1    |[adore, classical, music, #favorite] |(262144,[100089,102383,108624,131250],[1.0,1.0,1.0,1.0])|\n",
            "+-----+-------------------------------------+--------------------------------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JbsGNzaLtvt"
      },
      "source": [
        "#Predict testing data and calculate the accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B895BV3lKkm",
        "outputId": "8cf8d216-efc7-418e-fe0a-650cc9235e03"
      },
      "source": [
        "prediction = model.transform(numericTest)\n",
        "predictionFinal = prediction.select(\n",
        "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
        "predictionFinal.show(n=4, truncate = False)\n",
        "correctPrediction = predictionFinal.filter(\n",
        "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
        "totalData = predictionFinal.count()\n",
        "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
        "      \", accuracy:\", correctPrediction/totalData)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------+----------+-----+\n",
            "|MeaningfulWords                      |prediction|Label|\n",
            "+-------------------------------------+----------+-----+\n",
            "|[adore, classical, music, #brilliant]|1.0       |1    |\n",
            "|[adore, classical, music, #favorite] |1.0       |1    |\n",
            "|[adore, coffee, #favorite]           |1.0       |1    |\n",
            "|[adore, coffee, #loveit]             |1.0       |1    |\n",
            "+-------------------------------------+----------+-----+\n",
            "only showing top 4 rows\n",
            "\n",
            "correct prediction: 594 , total data: 605 , accuracy: 0.9818181818181818\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}